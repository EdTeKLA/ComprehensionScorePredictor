{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pickle\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.parse import CoreNLPParser\n",
    "import statistics\n",
    "\n",
    "from word_freq import preprocess\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade = \"3\"\n",
    "target_path = f\"../../data/question_correctness/.csv\"\n",
    "scores_path = \"../../Gates.ReadComp_By-Item_Gr3-5(CM).xlsx\"\n",
    "corpus_path = f\"../../subtest_txt/gr{grade}_paragraphs.txt\"\n",
    "questions_path = f\"../../subtest_txt/gr{grade}_questions.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtest Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Gr3.RC.Gates_01', 'Gr3.RC.Gates_02', 'Gr3.RC.Gates_03',\n",
      "       'Gr3.RC.Gates_04', 'Gr3.RC.Gates_05', 'Gr3.RC.Gates_06',\n",
      "       'Gr3.RC.Gates_07', 'Gr3.RC.Gates_08', 'Gr3.RC.Gates_09',\n",
      "       'Gr3.RC.Gates_10',\n",
      "       ...\n",
      "       'Gr5.RC.Gates_45', 'Gr5.RC.Gates_46', 'Gr5.RC.Gates_47',\n",
      "       'Gr5.RC.Gates_48', 'Gr5.RC.Gates_RawScore', 'Gr5.RC.Gates_GradeEquiv',\n",
      "       'Gr5.RC.Gates_NCE', 'Gr5.RC.Gates_NPR', 'Gr5.RC.Gates_NS',\n",
      "       'Gr5.RC.Gates_ESS'],\n",
      "      dtype='object', length=162)\n"
     ]
    }
   ],
   "source": [
    "sub_tests = []\n",
    "\n",
    "with open(corpus_path,'r') as fp:\n",
    "    sub_test = fp.readline()\n",
    "    while sub_test:\n",
    "        sub_tests.append(sub_test)\n",
    "        sub_test = fp.readline()\n",
    "\n",
    "\n",
    "df = pd.read_excel(scores_path)\n",
    "sub_test_number = df.columns[1:]\n",
    "\n",
    "print(sub_test_number)\n",
    "\n",
    "questions_ranges = [(1,5), (6,8), (9,13), (14,16), (17,21), (22,27), (28,30),\n",
    "                   (31,35), (36,40), (41,43), (44,48)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "\n",
    "#Change the length to 500\n",
    "model.max_seq_length = 500\n",
    "\n",
    "sub_tests_embed = model.encode(sub_tests, show_progress_bar=True)\n",
    "\n",
    "print(sub_tests_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtest rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4358974358974359, 0.4146341463414634, 0.41818181818181815, 0.5161290322580645, 0.5, 0.55, 0.42424242424242425, 0.5660377358490566, 0.45098039215686275, 0.47368421052631576, 0.4583333333333333]\n",
      "0.4734655026169795\n"
     ]
    }
   ],
   "source": [
    "sub_tests_rare = []\n",
    "def get_rare_words_perc(text,wordlist):\n",
    "    tokens = preprocess(text)\n",
    "    counter = 0\n",
    "    for t in tokens:\n",
    "        if t not in wordlist:\n",
    "            counter += 1\n",
    "    return counter/len(tokens)\n",
    "\n",
    "with open(\"wordlist.pkl\",'rb') as fp:\n",
    "    wordlist = pickle.load(fp)\n",
    "\n",
    "for st in sub_tests:\n",
    "    sub_tests_rare.append(get_rare_words_perc(st, wordlist))\n",
    "    \n",
    "print(sub_tests_rare)\n",
    "print(sum(sub_tests_rare)/len(sub_tests_rare))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4734655026169795\n",
      "0.05048645170960405\n",
      "0.4146341463414634\n",
      "0.5660377358490566\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.mean(sub_tests_rare,axis=0))\n",
    "print(np.std(sub_tests_rare,axis=0))\n",
    "print(np.min(sub_tests_rare,axis=0))\n",
    "print(np.max(sub_tests_rare,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"awl.txt\",'r') as file:\n",
    "    corpus = file.read().replace('\\n', ' ')\n",
    "tokens = preprocess(corpus)\n",
    "freq = Counter(tokens)\n",
    "print(freq)\n",
    "rare_list = freq.keys()\n",
    "\n",
    "for i in range(len(sub_tests)):\n",
    "    print(i)\n",
    "    tokens = preprocess(sub_tests[i])\n",
    "    counter = 0\n",
    "    for t in tokens:\n",
    "        if t in rare_list:\n",
    "            print(t)\n",
    "            counter += 1\n",
    "    print('total:', counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate parse tree depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_depth(parser,text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "#     print(sentences)\n",
    "    depths = []\n",
    "    for s in sentences:\n",
    "        parse = next(parser.raw_parse(s))\n",
    "#         parse.draw()\n",
    "        depths.append(parse.height())\n",
    "        \n",
    "    return statistics.mean(depths)\n",
    "\n",
    "def get_depth_stats(parser,text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "#     print(sentences)\n",
    "    depths = []\n",
    "    for s in sentences:\n",
    "        parse = next(parser.raw_parse(s))\n",
    "#         parse.draw()\n",
    "        depths.append(parse.height())\n",
    "        \n",
    "    return [statistics.mean(depths), statistics.stdev(depths), max(depths)]\n",
    "#     return [min(depths), max(depths)]\n",
    "\n",
    "parser = CoreNLPParser(url='http://localhost:9000')\n",
    "\n",
    "# parse = next(parser.raw_parse(nltk.sent_tokenize('Snow turns blue when blue iceworms live in it.')[0]))\n",
    "# print(parse.height())\n",
    "# parse.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions and Answers Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "with open(questions_path,'r') as fp:\n",
    "    for _ in range(48):\n",
    "        question = fp.readline()\n",
    "        answer = fp.readline()\n",
    "#         print(question,answer)\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        \n",
    "print(len(questions))\n",
    "print(len(answers))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 768)\n",
      "(48, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions_embed = model.encode(questions, show_progress_bar=True)\n",
    "answers_embed = model.encode(answers, show_progress_bar=True)\n",
    "\n",
    "print(questions_embed.shape)\n",
    "print(answers_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "questions_depth = []\n",
    "answer_depth = []\n",
    "\n",
    "for text in sub_tests:\n",
    "    depths.append(get_depth_stats(parser,text))\n",
    "\n",
    "for q in questions:\n",
    "    questions_depth.append(get_average_depth(parser,q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 2.0, 12], [8.75, 3.1959796173138706, 15], [8, 1.3416407864998738, 11], [7.375, 1.9226098333849673, 11], [8.235294117647058, 2.305683514836378, 12], [10.333333333333334, 3.9619401430321606, 16], [8, 3.2145502536643185, 13], [9.571428571428571, 2.14919697074224, 14], [8.071428571428571, 2.234839031005476, 11], [8.75, 1.9086270308410553, 11], [7.181818181818182, 2.238970078627004, 12]]\n"
     ]
    }
   ],
   "source": [
    "print(depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[9, 2.0, 12, 7]\n",
      "6\n",
      "[9, 2.0, 12, 6]\n",
      "7\n",
      "[9, 2.0, 12, 7]\n",
      "6\n",
      "[9, 2.0, 12, 6]\n",
      "6\n",
      "[9, 2.0, 12, 6]\n",
      "7\n",
      "[8.75, 3.1959796173138706, 15, 7]\n",
      "9\n",
      "[8.75, 3.1959796173138706, 15, 9]\n",
      "6\n",
      "[8.75, 3.1959796173138706, 15, 6]\n",
      "7\n",
      "[8, 1.3416407864998738, 11, 7]\n",
      "9\n",
      "[8, 1.3416407864998738, 11, 9]\n",
      "8\n",
      "[8, 1.3416407864998738, 11, 8]\n",
      "8\n",
      "[8, 1.3416407864998738, 11, 8]\n",
      "10\n",
      "[8, 1.3416407864998738, 11, 10]\n",
      "9\n",
      "[7.375, 1.9226098333849673, 11, 9]\n",
      "7\n",
      "[7.375, 1.9226098333849673, 11, 7]\n",
      "10\n",
      "[7.375, 1.9226098333849673, 11, 10]\n",
      "10\n",
      "[8.235294117647058, 2.305683514836378, 12, 10]\n",
      "6\n",
      "[8.235294117647058, 2.305683514836378, 12, 6]\n",
      "7\n",
      "[8.235294117647058, 2.305683514836378, 12, 7]\n",
      "8\n",
      "[8.235294117647058, 2.305683514836378, 12, 8]\n",
      "8\n",
      "[8.235294117647058, 2.305683514836378, 12, 8]\n",
      "8\n",
      "[10.333333333333334, 3.9619401430321606, 16, 8]\n",
      "10\n",
      "[10.333333333333334, 3.9619401430321606, 16, 10]\n",
      "11\n",
      "[10.333333333333334, 3.9619401430321606, 16, 11]\n",
      "7\n",
      "[10.333333333333334, 3.9619401430321606, 16, 7]\n",
      "9\n",
      "[10.333333333333334, 3.9619401430321606, 16, 9]\n",
      "11\n",
      "[10.333333333333334, 3.9619401430321606, 16, 11]\n",
      "12\n",
      "[8, 3.2145502536643185, 13, 12]\n",
      "13\n",
      "[8, 3.2145502536643185, 13, 13]\n",
      "8\n",
      "[8, 3.2145502536643185, 13, 8]\n",
      "6\n",
      "[9.571428571428571, 2.14919697074224, 14, 6]\n",
      "9\n",
      "[9.571428571428571, 2.14919697074224, 14, 9]\n",
      "6\n",
      "[9.571428571428571, 2.14919697074224, 14, 6]\n",
      "12\n",
      "[9.571428571428571, 2.14919697074224, 14, 12]\n",
      "8\n",
      "[9.571428571428571, 2.14919697074224, 14, 8]\n",
      "10\n",
      "[8.071428571428571, 2.234839031005476, 11, 10]\n",
      "11\n",
      "[8.071428571428571, 2.234839031005476, 11, 11]\n",
      "8\n",
      "[8.071428571428571, 2.234839031005476, 11, 8]\n",
      "10\n",
      "[8.071428571428571, 2.234839031005476, 11, 10]\n",
      "10\n",
      "[8.071428571428571, 2.234839031005476, 11, 10]\n",
      "6\n",
      "[8.75, 1.9086270308410553, 11, 6]\n",
      "6\n",
      "[8.75, 1.9086270308410553, 11, 6]\n",
      "7\n",
      "[8.75, 1.9086270308410553, 11, 7]\n",
      "8\n",
      "[7.181818181818182, 2.238970078627004, 12, 8]\n",
      "7\n",
      "[7.181818181818182, 2.238970078627004, 12, 7]\n",
      "7\n",
      "[7.181818181818182, 2.238970078627004, 12, 7]\n",
      "8\n",
      "[7.181818181818182, 2.238970078627004, 12, 8]\n",
      "13\n",
      "[7.181818181818182, 2.238970078627004, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "complexity = []\n",
    "for index, q_range in enumerate(questions_ranges):\n",
    "    entry = list(depths[index])\n",
    "    for j in range(q_range[0],q_range[1]+1):\n",
    "        new_entry = entry.copy()\n",
    "        new_entry.append(questions_depth[j-1])\n",
    "        print(questions_depth[j-1])\n",
    "        print(new_entry)\n",
    "        \n",
    "        complexity.append(new_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDA, no need to run\n",
    "complexity = np.asarray(complexity)\n",
    "complexity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.11015698339985273, 0.4560630509727088)\n"
     ]
    }
   ],
   "source": [
    "# EDA, no need to run\n",
    "from scipy.stats.stats import pearsonr\n",
    "print(pearsonr(complexity[:,2],complexity[:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7155376100784874, 0.013292661910529524)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDA, no need to run\n",
    "depths = np.asarray(depths)\n",
    "from scipy.stats.stats import pearsonr\n",
    "print(pearsonr(depths[:,0],depths[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.          8.75        8.          7.375       8.23529412 10.33333333\n",
      "  8.          9.57142857  8.07142857  8.75        7.18181818]\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "#interactive plotting in separate window\n",
    "%matplotlib qt \n",
    "\n",
    "depths_arr = np.asarray(depths)[:,0]\n",
    "print(depths_arr)\n",
    "print(len(sub_tests))\n",
    "plt.plot(np.arange(1,12),depths_arr,'--',label='Mean',color='#757559')\n",
    "plt.fill_between(np.arange(1,12), depths_arr - np.asarray(depths)[:,1],\n",
    "                 depths_arr + np.asarray(depths)[:,1], color=\"#d6d6a3\")#F5F5BA\n",
    "plt.plot(np.arange(1,12),np.asarray(depths)[:,2],color='#757559', label='Max')\n",
    "plt.xticks(np.arange(1, 12, step=1))\n",
    "plt.yticks(np.arange(3, 17, step=1))\n",
    "plt.xlabel('Text Number')\n",
    "plt.ylabel('Sentence Complexity')\n",
    "plt.tight_layout()\n",
    "# plt.title('Subtest-wise Sentence Complexity Distribution')\n",
    "plt.ylim(3,17)\n",
    "# plt.xlim(1,11)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.375\n",
      "1.943203969393503\n",
      "6\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "new_depth = np.asarray(depths)\n",
    "\n",
    "print(np.mean(questions_depth,axis=0))\n",
    "print(np.std(questions_depth,axis=0))\n",
    "print(np.min(questions_depth,axis=0))\n",
    "print(np.max(questions_depth,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 3)\n",
      "[0.44681111 0.49253705 0.56750447]\n",
      "[0.30261789 0.26272334 0.30222353]\n",
      "[0. 0. 0.]\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "surprisal = pickle.load(open('surprisal.pkl','rb'))\n",
    "surprisal = np.asarray(surprisal)\n",
    "print(surprisal.shape)\n",
    "\n",
    "for i in range(surprisal.shape[1]):\n",
    "    v = surprisal[:, i]   # foo[:, -1] for the last column\n",
    "    surprisal[:, i] = (v - v.min()) / (v.max() - v.min())\n",
    "print(np.mean(surprisal,axis=0))\n",
    "print(np.std(surprisal,axis=0))\n",
    "print(np.min(surprisal,axis=0))\n",
    "print(np.max(surprisal,axis=0))\n",
    "surprisal = surprisal.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprisal = pickle.load(open('surprisal.pkl','rb'))\n",
    "surprisal = np.asarray(surprisal)\n",
    "surprisal_mean = np.asarray(surprisal)[1:,0]\n",
    "surprisal_std = np.asarray(surprisal)[1:,1]\n",
    "surprisal_max = np.asarray(surprisal)[1:,2]\n",
    "\n",
    "plt.plot(np.arange(1,12),surprisal_mean,'--',label='Mean',color='#757559')\n",
    "plt.fill_between(np.arange(1,12), surprisal_mean - surprisal_std,\n",
    "                 surprisal_mean + surprisal_std, color=\"#d6d6a3\")\n",
    "plt.plot(np.arange(1,12),surprisal_max,color='#757559', label='Max')\n",
    "plt.xticks(np.arange(1, 12, step=1))\n",
    "plt.xlabel('Text Number')\n",
    "plt.ylabel('Lexical Surprisal')\n",
    "# plt.title('Subtest-wise Lexical Surprisal Distribution')\n",
    "plt.ylim(0,0.140)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path = \"../../SoR_Alberta.Shared.Data.and.Codebook.xlsx\"#\"../data/gr3/gr3_features.xlsx\"\n",
    "feature_names = ['G3.PPVT.Vocab.raw',\n",
    "                 'G3.Elision.PA.raw',\n",
    "                 'G3.Syn.GramCorrect.raw',\n",
    "                 'G3.TOWRE.PDE.raw',\n",
    "                 'G3.DigitSpan.raw',\n",
    "                 'G3.WordID.raw',\n",
    "                ]\n",
    "\n",
    "df2 = pd.read_excel(feature_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove non-full entries\n",
    "for name in feature_names:\n",
    "    unavailable_index = df2[df2[name]<0].index\n",
    "    df2.drop(unavailable_index , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add towre scores\n",
    "df2['G3.TOWRE.raw'] = df2['G3.TOWRE.SWE.raw'] + df2['G3.TOWRE.PDE.raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G3.PPVT.Vocab.raw\n",
      "count    138.000000\n",
      "mean      31.949275\n",
      "std        4.930647\n",
      "min       19.000000\n",
      "25%       29.000000\n",
      "50%       32.000000\n",
      "75%       36.000000\n",
      "max       46.000000\n",
      "Name: G3.PPVT.Vocab.raw, dtype: float64\n",
      "G3.Elision.PA.raw\n",
      "count    138.000000\n",
      "mean      23.014493\n",
      "std        6.097727\n",
      "min        8.000000\n",
      "25%       17.250000\n",
      "50%       25.000000\n",
      "75%       28.000000\n",
      "max       33.000000\n",
      "Name: G3.Elision.PA.raw, dtype: float64\n",
      "G3.Syn.GramCorrect.raw\n",
      "count    138.000000\n",
      "mean       9.565217\n",
      "std        3.266634\n",
      "min        2.000000\n",
      "25%        7.000000\n",
      "50%       10.000000\n",
      "75%       12.000000\n",
      "max       16.000000\n",
      "Name: G3.Syn.GramCorrect.raw, dtype: float64\n",
      "G3.TOWRE.PDE.raw\n",
      "count    138.000000\n",
      "mean      24.471014\n",
      "std       12.803508\n",
      "min        0.000000\n",
      "25%       15.000000\n",
      "50%       24.500000\n",
      "75%       34.000000\n",
      "max       51.000000\n",
      "Name: G3.TOWRE.PDE.raw, dtype: float64\n",
      "G3.DigitSpan.raw\n",
      "count    138.000000\n",
      "mean      13.173913\n",
      "std        2.308072\n",
      "min        8.000000\n",
      "25%       12.000000\n",
      "50%       13.000000\n",
      "75%       15.000000\n",
      "max       20.000000\n",
      "Name: G3.DigitSpan.raw, dtype: float64\n",
      "G3.WordID.raw\n",
      "count    138.000000\n",
      "mean      60.971014\n",
      "std       12.723874\n",
      "min        3.000000\n",
      "25%       55.250000\n",
      "50%       63.000000\n",
      "75%       70.000000\n",
      "max       83.000000\n",
      "Name: G3.WordID.raw, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for name in feature_names:\n",
    "    print(name)\n",
    "    print(df2[name].describe())\n",
    "\n",
    "# normalize all feature using min-max normalization\n",
    "normalized_df2=(df2-df2.min())/(df2.max()-df2.min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble all features into one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.51851852  0.72        0.21428571  0.43137255  0.25        0.7\n",
      "  9.          2.         12.          0.47353246  0.46016297  0.44980291\n",
      "  0.43589744  7.        ]\n",
      "(4, 17)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "\n",
    "combined = []\n",
    "\n",
    "for i in normalized_df2.index:\n",
    "\n",
    "    skills = []\n",
    "    # Retrieve all skill feature data\n",
    "    for name in feature_names:\n",
    "        value = normalized_df2[name][i]\n",
    "        skills.append(value)\n",
    "    \n",
    "    for index, q_range in enumerate(questions_ranges):\n",
    "        entry = []\n",
    "        entry.append(skills+depths[index]+surprisal[index]+[sub_tests_rare[index]]) ### append depth and rare word feature\n",
    "        entry.append(sub_tests_embed[index])\n",
    "        \n",
    "        for j in range(q_range[0],q_range[1]+1):\n",
    "            # add embeddings\n",
    "            detail = []\n",
    "            detail.append(questions_embed[j-1])\n",
    "            # add cfg depth\n",
    "            new_entry = copy.deepcopy(entry)\n",
    "\n",
    "            new_entry[0] += [questions_depth[j-1]]\n",
    "            new_entry[0] = np.asarray(new_entry[0])\n",
    "            \n",
    "            if df[f\"Gr{grade}.RC.Gates_\"+\"{:02d}\".format(j)][i] == 1:\n",
    "                detail.append(1)\n",
    "            elif df[f\"Gr{grade}.RC.Gates_\"+\"{:02d}\".format(j)][i] in (2,0):\n",
    "                detail.append(0)\n",
    "            else:\n",
    "                print('DNE')\n",
    "            \n",
    "            # skill features,sentence embedding, label, (reading number, question number)\n",
    "            data.append(new_entry+detail+[(index,j)]) \n",
    "\n",
    "print(data[0][0])\n",
    "print(data[400][4])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save pre-computed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pkl\",'wb') as fp:\n",
    "    pickle.dump(data, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pkl\",'rb') as fp:\n",
    "    new_data = pickle.load(fp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "528752051969e0bd20ab5259c6834cf3ee5d9c17a4d6409bc9d5bfcb9542e87d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
